{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código realiza una serie de operaciones de procesamiento y partición de datos utilizando las bibliotecas `pandas` y `numpy`. Aquí está el resumen de las acciones principales que realiza el código:\n",
    "\n",
    "1. **Carga de Datos**: \n",
    "   - Se importan las bibliotecas `numpy` y `pandas`.\n",
    "   - Se carga un archivo CSV (`datos_limpiados.csv`) en un DataFrame llamado `datos`.\n",
    "\n",
    "2. **Preparación de Datos**:\n",
    "   - Se establece la columna 'Time' como el índice del DataFrame y se convierte a un índice de fecha y hora.\n",
    "   - Se eliminan las columnas no numéricas, en este caso, la columna 'Date'.\n",
    "   - Los datos se resamplean a nivel horario tomando la media de cada hora.\n",
    "\n",
    "3. **Limpieza de Datos**:\n",
    "   - Se elimina la última fila del DataFrame.\n",
    "   - Se verifica si hay valores vacíos en el DataFrame.\n",
    "   - Se completan los valores vacíos utilizando la técnica de 'forward fill' y 'backward fill'.\n",
    "   - Se verifica nuevamente si hay valores vacíos después de la imputación.\n",
    "\n",
    "4. **Guardar Datos Preprocesados**:\n",
    "   - Los datos readaptados se guardan en un nuevo archivo CSV (`datos_readaptados.csv`).\n",
    "\n",
    "5. **Partición de Datos**:\n",
    "   - Se seleccionan datos dentro de un rango de fechas específico (desde '2012-01-01 00:00:00' hasta '2014-12-30 23:00:00').\n",
    "   - Se calcula el tamaño total de los datos y se divide en tres conjuntos: entrenamiento (70%), validación (15%) y prueba (15%).\n",
    "   - Se determinan las fechas límite para cada conjunto ajustando las horas a las 23:00 horas para evitar divisiones a mitad del día.\n",
    "   - Se crean particiones de datos para entrenamiento, validación y prueba.\n",
    "   - Se eliminan las primeras entradas de los conjuntos de validación y prueba para evitar solapamiento.\n",
    "\n",
    "6. **Conversión de Tipos de Datos**:\n",
    "   - Se mapean valores específicos en la columna 'Holiday' a valores booleanos (0.0 a False y 1.0 a True).\n",
    "   - Se asegura la tipificación booleana de la columna 'Holiday'.\n",
    "\n",
    "7. **Verificación y Guardado de Particiones**:\n",
    "   - Se muestran los tamaños de los conjuntos de entrenamiento, validación y prueba.\n",
    "   - Se imprimen las fechas límite para verificación.\n",
    "   - Se muestran las últimas filas del conjunto de entrenamiento y validación, y las primeras filas del conjunto de prueba.\n",
    "   - Finalmente, cada conjunto se guarda en un archivo CSV separado (`train.csv`, `val.csv` y `test.csv`).\n",
    "\n",
    "El código asegura que los datos están adecuadamente preparados y particionados para ser utilizados en un modelo de aprendizaje automático o análisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arzua\\AppData\\Local\\Temp\\ipykernel_7012\\568598159.py:13: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  datos= datos.resample('H', closed='left', label='right').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hay valores vacíos en el dataset? True\n",
      "¿Hay valores vacíos en el dataset después de completar? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arzua\\AppData\\Local\\Temp\\ipykernel_7012\\568598159.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  datos = datos.fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\arzua\\AppData\\Local\\Temp\\ipykernel_7012\\568598159.py:24: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  datos = datos.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 18407\n",
      "Tamaño del conjunto de validación: 3936\n",
      "Tamaño del conjunto de prueba: 3936\n",
      "Fecha fin entrenamiento: 2014-02-05 23:00:00\n",
      "Fecha fin validación: 2014-07-19 23:00:00\n",
      "                          Demand  Temperature  Holiday\n",
      "Time                                                  \n",
      "2014-02-05 19:00:00  6038.798486        29.60    False\n",
      "2014-02-05 20:00:00  5700.070189        27.55    False\n",
      "2014-02-05 21:00:00  5389.295345        25.55    False\n",
      "2014-02-05 22:00:00  5191.951051        23.65    False\n",
      "2014-02-05 23:00:00  4671.543949        22.95    False\n",
      "                          Demand  Temperature  Holiday\n",
      "Time                                                  \n",
      "2014-07-19 19:00:00  5647.680616        11.55    False\n",
      "2014-07-19 20:00:00  5377.088537        11.15    False\n",
      "2014-07-19 21:00:00  5135.699017        11.00    False\n",
      "2014-07-19 22:00:00  4856.480564        10.90    False\n",
      "2014-07-19 23:00:00  4660.067225        10.65    False\n",
      "                          Demand  Temperature  Holiday\n",
      "Time                                                  \n",
      "2014-07-20 00:00:00  4940.189041        10.50    False\n",
      "2014-07-20 01:00:00  4673.333498        10.25    False\n",
      "2014-07-20 02:00:00  4206.513976        10.00    False\n",
      "2014-07-20 03:00:00  3820.840831         9.85    False\n",
      "2014-07-20 04:00:00  3625.199264         9.35    False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "datos=pd.read_csv(r\"C:\\Users\\arzua\\OneDrive\\Escritorio\\preelec\\src\\data\\datos_limpiados.csv\")\n",
    "\n",
    "#establecer time como indice\n",
    "\n",
    "datos = datos.set_index('Time')\n",
    "datos.index = pd.to_datetime(datos.index)\n",
    "# Seleccionar solo las columnas numéricas\n",
    "datos = datos.drop(columns='Date')\n",
    "\n",
    "# Resamplear los datos a nivel horario sin introducir información futura\n",
    "datos= datos.resample('H', closed='left', label='right').mean()\n",
    "\n",
    "datos\n",
    "\n",
    "#eliminacion del ultimo\n",
    "datos=datos.iloc[:-1]\n",
    "datos\n",
    "# Comprobar si hay valores vacíos\n",
    "print(\"¿Hay valores vacíos en el dataset?\", datos.isnull().values.any())\n",
    "\n",
    "# Completar valores vacíos con la media ponderada (forward fill y backward fill)\n",
    "datos = datos.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Verificar de nuevo\n",
    "print(\"¿Hay valores vacíos en el dataset después de completar?\", datos.isnull().values.any())\n",
    "#guardar datos readaptados finales\n",
    "datos.to_csv(r\"C:\\Users\\arzua\\OneDrive\\Escritorio\\preelec\\src\\data\\datos_readaptados.csv\")\n",
    "import pandas as pd\n",
    "\n",
    "# Suponiendo que 'datos' es un DataFrame con un índice de fecha y hora\n",
    "datos = datos.loc['2012-01-01 00:00:00':'2014-12-30 23:00:00'].copy()\n",
    "\n",
    "# Calcular el tamaño de cada conjunto\n",
    "total_size = len(datos)\n",
    "train_size = int(0.70 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Obtener las fechas límite basadas en el tamaño calculado, ajustando a las 23:00 horas\n",
    "fin_train = datos.index[train_size - 1].replace(hour=23)\n",
    "fin_val = datos.index[train_size + val_size - 1].replace(hour=23)\n",
    "\n",
    "# Crear las particiones de datos\n",
    "datos_train = datos.loc[:fin_train].copy()\n",
    "datos_val = datos.loc[fin_train:fin_val].copy()\n",
    "datos_test = datos.loc[fin_val:].copy()\n",
    "\n",
    "# Eliminar la primera entrada de los conjuntos de validación y prueba\n",
    "datos_val = datos_val.iloc[1:].copy()\n",
    "datos_test = datos_test.iloc[1:].copy()\n",
    "\n",
    "# Mostrar tamaños de los conjuntos\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(datos_train))\n",
    "print(\"Tamaño del conjunto de validación:\", len(datos_val))\n",
    "print(\"Tamaño del conjunto de prueba:\", len(datos_test))\n",
    "\n",
    "# Mostrar las fechas límite para verificación\n",
    "print(\"Fecha fin entrenamiento:\", fin_train)\n",
    "print(\"Fecha fin validación:\", fin_val)\n",
    "\n",
    "# Mapear valores específicos a booleanos (suponiendo 0.0 y 1.0 como False y True respectivamente)\n",
    "datos_train['Holiday'] = datos_train['Holiday'].map({0.0: False, 1.0: True})\n",
    "datos_val['Holiday'] = datos_val['Holiday'].map({0.0: False, 1.0: True})\n",
    "datos_test['Holiday'] = datos_test['Holiday'].map({0.0: False, 1.0: True})\n",
    "\n",
    "# Convertir a booleano (esto es redundante ya que el mapeo ya lo convierte a bool, pero se deja para asegurar la tipificación)\n",
    "datos_train['Holiday'] = datos_train['Holiday'].astype(bool)\n",
    "datos_val['Holiday'] = datos_val['Holiday'].astype(bool)\n",
    "datos_test['Holiday'] = datos_test['Holiday'].astype(bool)\n",
    "\n",
    "# Mostrar los primeros registros para verificar\n",
    "print(datos_train.tail())\n",
    "print(datos_val.tail())\n",
    "print(datos_test.head())\n",
    "\n",
    "# Guardar cada conjunto en un archivo CSV separado\n",
    "datos_train.to_csv(r\"C:\\Users\\arzua\\OneDrive\\Escritorio\\preelec\\src\\data\\train.csv\")\n",
    "datos_val.to_csv(r\"C:\\Users\\arzua\\OneDrive\\Escritorio\\preelec\\src\\data\\val.csv\")\n",
    "datos_test.to_csv(r\"C:\\Users\\arzua\\OneDrive\\Escritorio\\preelec\\src\\data\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_elec_1)",
   "language": "python",
   "name": "nombre_del_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
